architectures: &Arch
  model:
    select: 'TemplateModel'
    TemplateModel:
      name: 'TemplateModel'
      module: 'custom.model.template'
      kwargs:
        input_size: 128
        num_classes: &N 8
        hidden_size: 256

  optimizer:
    select: 'AdamW'
    AdamW: &AdamW_args
      name: 'AdamW'
      module: 'torch.optim'
      kwargs:
        lr: 0.0003

  scheduler:
    select: 'ExponentialLR'
    ExponentialLR:
      name: 'ExponentialLR'
      module: 'torch.optim.lr_scheduler'
      kwargs:
        gamma: 0.9995
    ConstantLR:
      name: 'ConstantLR'
      module: 'torch.optim.lr_scheduler'
      kwargs:
        total_iters: 0

  loss:
    select: 'CrossEntropyLoss'
    CrossEntropyLoss:
      name: 'CrossEntropyLoss'
      module: 'torch.nn'
      kwargs: {}

  metric:
    select:
      - 'MulticlassAccuracy'
    use_loss: True
    MulticlassAccuracy:
      name: 'MulticlassAccuracy'
      module: 'torchmetrics.classification'
      kwargs:
        num_classes: *N
        average: 'macro'


data:
  datasets:
    train_hdf5_template:
      name: 'HDF5DataSet'
      module: 'custom.dataset.hdf5_dataset'
      kwargs:
        dataset_path: './data/processed/template/train.h5'
    valid_hdf5_template:
      name: 'HDF5DataSet'
      module: 'custom.dataset.hdf5_dataset'
      kwargs:
        dataset_path: './data/processed/template/valid.h5'
    test_hdf5_template:
      name: 'HDF5DataSet'
      module: 'custom.dataset.hdf5_dataset'
      kwargs:
        dataset_path: './data/processed/template/test.h5'

  dataloaders:
    train_loader:
      kwargs:
        batch_size: 32
        shuffle: True
        pin_memory: True
        num_workers: 0
      dataset: 'train_hdf5_template'
    valid_loader:
      kwargs:
        batch_size: 1
        shuffle: False
        pin_memory: True
        num_workers: 0
      dataset: 'valid_hdf5_template'
    test_loader:
      kwargs:
        batch_size: 1
        shuffle: False
        pin_memory: True
        num_workers: 0
      dataset: 'test_hdf5_template'


runner:
  step_mod: 'grad' # 'epoch' or 'grad'
  total_steps: 1100
  log_freq: 100
  dev_freq: 1000
  save_freq: 500
  trainer_kwargs:
    gradient_accumulation_steps: 1
  valider_kwargs: {}


ckpt:
  save_mod : 'max'
  check_metrics:
    - 'MulticlassAccuracy'
  ckpt_dir: ~  # default: './results/{name}'
  keep_num: 3


WandB:
  project: &P 'Template_project'
  init_kwargs:
    config: *Arch
  watch_kwargs:
    log: 'all'
    log_freq: 1000

